apiVersion: v1
kind: Service
metadata:
  name: tf-serving-gpu
  namespace: default
spec:
  ports:
  - name: grpc
    port: 9000
    nodePort: 30901
    targetPort: 9000
  selector:
    app: tf-serving-gpu
  type: NodePort
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: tf-serving-gpu
  labels: 
    app: tf-serving-gpu
spec:
  replicas: 1
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: tf-serving-gpu
    spec:
      containers:
      - image: samnco/tf-serving:gpu
        name: tf-serving
        command: ["/entrypoint.sh"]
        imagePullPolicy: IfNotPresent
        env: 
        - name: LD_LIBRARY_PATH
          value: "$LD_LIBRARY_PATH:/usr/lib/nvidia:/usr/lib/cuda"
        - name: PORT
          value: "9000"
        - name: MODEL_NAME
          value: "inception"
        - name: MODEL_PATH
          value: "/serving/model-data"
        ports:
        - containerPort: 9000
        readinessProbe:
          tcpSocket:
            port: 9000
          timeoutSeconds: 5
        livenessProbe:
          tcpSocket:
            port: 9000
          initialDelaySeconds: 30
          timeoutSeconds: 5
          failureThreshold: 6
        resources: 
          requests: 
            cpu: 400m
            memory: 2048Mi
            alpha.kubernetes.io/nvidia-gpu: "1"
          limits: 
            cpu: 600m
            memory: 16384Mi
            alpha.kubernetes.io/nvidia-gpu: "1"
        volumeMounts:
        - mountPath: /serving/tf-serving
          name: tf-serving-data
        - mountPath: /serving/model-data
          name: tf-serving-seed-inception
        - mountPath: /usr/local/nvidia/bin
          name: bin
        - mountPath: /usr/lib/nvidia
          name: lib
        - mountPath: /usr/lib/cuda
          name: libcuda
      restartPolicy: Always
      nodeSelector: 
        kubernetes.io/gpu-name: nVidia-Tesla-P4
      volumes:
      - name: tf-serving-data
        hostPath: 
          path: /tf-data-gpu
          type: Directory
        # persistentVolumeClaim:
        #   claimName: tf-serving-data
      - name: tf-serving-seed-inception
        hostPath: 
          path: /seed
          type: Directory
        # persistentVolumeClaim:
        #   claimName: tf-serving-seed-inception
      - hostPath:
          path: /usr/lib/nvidia-387/bin
          type: ""
        name: bin
      - hostPath:
          path: /usr/lib/nvidia-387
          type: ""
        name: lib
      - hostPath:
          path: /usr/lib/x86_64-linux-gnu
          type: ""
        name: libcuda
---
apiVersion: v1
kind: Service
metadata:
  name: tf-resizer-gpu
  namespace: default
spec:
  ports:
  - name: http
    port: 5051
    targetPort: 5051
  selector:
    app: tf-image-resizer-gpu
  type: ClusterIP
# ---
# apiVersion: extensions/v1beta1
# kind: Deployment
# metadata:
#   name: tf-image-resizer-gpu
#   labels: 
#     app: tf-image-resizer-gpu
# spec:
#   replicas: 1
#   strategy:
#     type: Recreate
#   template:
#     metadata:
#       labels:
#         app: tf-image-resizer-gpu
#     spec:
#       containers:
#       - image: ronhanson/tensorflow-image-resizer:latest
#         name: tf-image-resizer
#         imagePullPolicy: Always
#         env: 
#         - name: TF_SERVER_PORT
#           value: "9000"
#         - name: TF_SERVER_NAME
#           value: tf-serving-gpu.default.svc
#         ports:
#         - containerPort: 5051
#         readinessProbe:
#           tcpSocket:
#             port: 5051
#           timeoutSeconds: 5
#         livenessProbe:
#           tcpSocket:
#             port: 5051
#           initialDelaySeconds: 30
#           timeoutSeconds: 5
#           failureThreshold: 6
#         resources: 
#           requests: 
#             cpu: 100m
#             memory: 1024Mi
#           limits: 
#             cpu: 500m
#             memory: 2014Mi            
#       restartPolicy: Always
# ---
